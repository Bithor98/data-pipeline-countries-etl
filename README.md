# Pipeline de Datos â€“ API a Base de Datos SQL

## ğŸ“Œ DescripciÃ³n del proyecto
Este proyecto implementa un **pipeline de datos sencillo pero realista**, orientado a demostrar
conocimientos de **ingenierÃ­a de datos a nivel junior**.

El sistema:
- Extrae datos desde una API pÃºblica
- Limpia y transforma los datos con Python
- Almacena la informaciÃ³n procesada en un formato estructurado
- Sigue un flujo ETL (Extract, Transform, Load)

Este proyecto forma parte de mi portfolio tÃ©cnico y estÃ¡ pensado para prÃ¡cticas
y primeras oportunidades profesionales en el sector tecnolÃ³gico.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas
- Python 3
- Requests
- Pandas
- SQLite (fase inicial)
- Git y GitHub

---

## ğŸ“‚ Estructura del proyecto



data-pipeline-project/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â””â”€â”€ load.py
â””â”€â”€ data/
â””â”€â”€ database.db



---

## ğŸ”„ Flujo del pipeline (ETL)

1. **ExtracciÃ³n (Extract)**  
   Se consumen datos desde una API REST pÃºblica.

2. **TransformaciÃ³n (Transform)**  
   Se limpian los datos, se normalizan columnas y se preparan para su almacenamiento.

3. **Carga (Load)**  
   Los datos se almacenan en una base de datos relacional.

---

## ğŸ¯ Objetivos del proyecto
- Practicar flujos reales de trabajo con datos
- Aplicar Python y SQL de forma conjunta
- Construir un proyecto sÃ³lido para el portfolio profesional

---

## ğŸš€ Posibles mejoras futuras
- Sustituir SQLite por PostgreSQL
- AÃ±adir contenedores Docker
- Incluir control de errores y logs
- Automatizar la ejecuciÃ³n del pipeline